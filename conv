cnn卷积的解释
卷积神经网络2016/11/13
——EddyGao
开胃菜：

（CNN最关键的问题，为什么要卷积？CNN为什么有这样的结构？为什么一遍遍的卷积后计算机就能实现分类或者识别了呢？）

首先，图像通过卷积会把图像的某种特征凸显出来，比如利用一个人们设计好的卷积核算子可以提取或者说凸显出图像的边缘信息，得到新的突出边缘的图像；利用一个人们设计好的卷积核算子可以提取或者凸显出纹理信息，得到新的纹理的图像。
	哇，原来卷积这么厉害，对图像进行卷积可以得到这么多信息~~~
	但是呢，这些卷积核是人们事先定义好的，是经过算法研究人员精心设计的，他们发现这样那样的设计卷积核对图像进行卷积运算可以突出一个什么样的特征，于是高高兴兴的拿去卷积了~~~然而，现在我们需要的这种特征太高级了，而且随着任务的不同，人工设计这些卷积核非常困难，于是，利用机器学习的思想，让计算机自己学习出卷积核来！！！

	比如说，对于一个图像分类问题：

	我们人类要识别一只猫，我们会通过看图像中是否有猫头、猫尾巴、猫身子等，如果具备这些特征，人类就会判断这是一只猫，那么对于计算机来说它是怎么判断的呢，这些高级的语义特征怎么通过卷积核来提取呢？？？
	CNN登场，巴拉巴拉巴拉。。。潇洒的解决了这个问题~先看看他英俊的外表0.0

	有人就问了，为什么CNN有那么多的卷积核呢？这是因为，判断是否是一只猫，只有一个特征是不够的，比如仅仅有猫头是不够的，(我们可能以为是猫头鹰哈哈)，我们需要多个高级语义的组合，所以需要多个卷积核。
	有人又问了，那为什么CNN有那么多层呢？首先，应该要明白，猫的头是一个特征，但是对于充斥着像素点的图像来说，用几个卷积核直接判断存在一个猫头的还是太困难，怎么办？简单，把猫头也作为一个识别目标，比如猫头应该具有更底层的一些语义特征，比如应该有猫的眼睛、猫的耳朵、猫的鼻子等等。这些特征有的还是太高级了，没关系，继续向下寻找低级特征，一直到最低级的像素点，这样就构成了多层的神经网络。
	还没完呢！！！这也是最不好理解的一部分，虽然我们之前一直用一些我们人常见的语义特征做例子，但是实际上CNN会学习出猫头、猫尾巴、猫身然后经判定这是猫吗？显然我们的CNN完全不知道什么叫猫头、猫尾巴，也就是说，CNN学习到的特征根本不是我们人类理解的语义特征，而是一种抽象特征，这种特征要是给我们人来看完全无法理解，但是这些特征组合在一起计算机就会判定这是一只猫！关于这一点，确实有些难以理解，比如一个人判断猫是看看有没有猫头、猫身子、猫尾巴，但是另一个选取的特征就是有没有猫的毛，猫的爪子，还有的人更加奇怪，他会去通过这张图像里是不是有老鼠去判断，而我们的CNN，则是最奇怪的一个人，他使用了我们完全无法用语言形容的一系列特征去判断。

下面我们开始正式的介绍我们的CNN

1、 局部感知/卷积层

通过卷积代替全连接实现局部感知



2、 池化层

池化是一个下采样的过程，因为参数太多了，为了不至于使我们的系统太复杂，我们通过下采样使参数稀疏化




3、 激活函数

为什么要引入激活函数呢？

如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了。
正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络就有意义了（不再是输入的线性组合，可以逼近任意函数）

实际也就是说，引入非线性激活函数可以比线性表征更多的信息


决定了神经元是被抑制还是激活

那么，常见的激活函数有那些呢？


4、 例子__过程



5、 前向传播





6、 反向传播

BP算法为例，也是最流行的优化算法，或者说是参数修正方法






好啦，以上就是我对卷积神经网络的理解~~~

参考资料：
1、通俗理解卷积神经网络（cs231n与5月dl班课程笔记）
http://blog.csdn.net/real_myth/article/details/51824193
2、如何直观的解释back propagation算法？
https://www.zhihu.com/question/27239198?rf=24827633
3、卷积神经网络直观解释
https://www.zhihu.com/question/39022858
4、莫烦python教学，tensorflow视频1，什么是神经网络
http://v.youku.com/v_show/id_XMTU5NDc3MDQwOA==.html?f=27327189&from=y1.7-1.3
